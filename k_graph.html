<!DOCTYPE HTML>
<html>
	<head>
		<title>Portfolio-Vaibhav Jain</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Main -->
					<div id="main">
						<article>
							<h2><center>Generating Knowledge Graphs from PDF’s.</center></h2>
							<center><img src="images/KG.png" alt="" width="600" height="130" /></center>
							<ul class="actions">
								<li><a href="index.html" class="button">Home</a></li>
							</ul>
						</article>
						<article>
							<center>Please find the complete project report at the Link: <a href="https://www.mdsi.tum.de/fileadmin/w00cet/di-lab/pdf/Horvath_SS2022_Final_Report.pdf" target="_blank">Project Report</a></center>
							<p>This project was done in collaboration with <a href="https://www.horvath-partners.com/en/expertise/it-digital-data-analytics/data-science-steering-lab" target="_blank">Steering Lab-Horv´ath</a>. The main goal is to build an end to end pipeline that takes PDF's as input and generate a knowledge graph for the text in the PDF. Due to the nature of the problem it was possible to structure the project as a pipeline. An overview of the pipeline can be seen below.
							<center><img src="images/KG_pipeline.png" alt=""  /></center>
							Each part was first developed independently and then everything was merged in one code base to finally generate the knowledge graph.</p>
							<center><h3>Data</h3></center>
							<p>The text documents used as input are scientific papers in PDF format from the German Environment Agency (Umweltbundesamt), which are written in English as well as in German language. Their lengths vary from three to 40 pages. The results shown here are for the the paper called <a href="https://www.umweltbundesamt.de/sites/default/files/medien/479/publikationen/scopp_the_revision_of_the_reach_authorisation_and_restriction_system.pdf" target="_blank">"The Revision of the REACH Authorisation and Restriction System"</a></p>
							<center><h3>Abstract Extraction</h3></center>
							<p>Since the summary or abstract of a paper contains its most important information, a knowledge graph of only these paragraphs will suffice to get a good overview of the paper, while keeping the graph clear. One way is to use abstractive summarization on the text of the paper but as we are working with scientific papers, most of the papers already had a pre-defined summary or abstract section. These sections are already very well written and can be used directly. For the papers who dont have such section we use abstractive summarization to build a summary. The difficulty of extracting these sections lies in the fact that these are in PDF format. We used "fitz" library from PyMuPDF to extract text from PDF's which returns text as a block in which data is seperated based on fonts, colo and font sizes. This creates a problem as we cannot simply parse line by line and look for abstract. To tackle this problem, we use several steps of heuristics that we defined based on sample data.</p>
							<ul>
								<li>Count the number of words for each font size in the document. The font size with most words is the font size of paragraphs in the PDF.</li>
								<li>Larger font sizes than this are defined as headings and smaller sizes as sub-paragraphs.</li>
								<li> Search for key words, like ”Abstract” or ”Summary” for the English papers and ”Abstrakt” or ”Einleitung” for the ones in German language, in the headings that were extracted.</li>
								<li>The paragraphs following the header with a key word until the next heading is then taken as the summary.</li>
							</ul>
							<center><h3>Coreference Resolution</h3></center>
							<p>The next step in the pipeline is coreference resolution, an NLP technique needed when two or more expressions refer to the same entity. Coreference resolution finds all the entities with the same meaning, clusters them together and replaces each with their reference entities. By performing coreference resolution long-range dependencies in the text can be handled and any duplicate edges in the knowledge graph are avoided.</p>
							<center><img src="images/cor_res.png"></center>
							<center><h3>Entity Extraction</h3></center>
							Next step is to extract entities from the text. These entities will act as the nodes for the knowledge graph. To extract all possible entities we combine Deep learning based NER with a rule based approach. The steps involved in the part of the pipeline is shown below.
							<ol>
								<li>We used BERT based NER approach to extract all possible named entities</li>
								<li>As NER only extraxct named entity, a rule based approach is used to extract all other entities
									<ul>
										<li>Produce a dependency trees using the dependency parser from the spaCy library.</li>
										<li>Entity extraction rule: <b>"Extract the subject and all objects along with their modifiers, compound words and punctuation marks between them."</b></li>
										<li>Parse the dependency tree and extract words based on above rule.</li>
									</ul>
								</li>
								<li>Merge the entities extracted from both approach and remove any duplicate entities.</li>
							</ol>
							<center><h3> Relationship Extraction</h3></center>
							To extract relatioships we used a self supervised approach named <b>"SelfORE"</b>. The main steps involved in this approach is shown below.
							<ol>
								<li>Decide on the number of clusters for the number of relationships to be extracted.</li>
								<li>Use Adaptive clustering(Please read the report to see how it works) to assign each entity pair to a perticular cluster.</li>
								<li>Using Entity pair as input and assigned cluster as output, train a transformer model to predict the cluster assignment.</li>
								<li>The trained transformer model can than be used to assign cluster numbers to new entity pairs.</li>
								<li>As the model returns the cluster asignment and not the actual text for the relationship, we used N-gram approach between the two entiites to assign text to each cluster.</li>
							</ol>
							<p>We combine the self supervised approach with a rule based approach that marks the root word of the sentence as the relationship between the entities in the sentence to generate a better KG.</p>
							<center><h3>Knowledge Graph generation</h3></center>
							Once we have both entities and relationships we generate the KG by keeping entities as nodes and relationships as edges.
							<center><img src="images/final_KG.png"></center>
						</article> 
					</div>

				<!-- Footer -->
					<footer id="footer">
						<section class="split contact">
							<section>
								<h3>Email</h3>
								<p>vaibhav.jain174@gmail.com</p>
							</section>
							<section>
								<h3>Socials</h3>
								<ul class="icons alt">
									<li><a href="https://linkedin.com/in/vaibhav-j-0b72a513a/" class="icon brands fa-linkedin" target="_blank"><span class="label">LinkedIn</span></a></li>
									<li><a href="https://www.facebook.com/vaibhav.b.jain.7/" class="icon brands alt fa-facebook-f" target="_blank"><span class="label">Facebook</span></a></li>
									<li><a href="https://www.instagram.com/vaibs12/" class="icon brands alt fa-instagram" target="_blank"><span class="label">Instagram</span></a></li>
									<li><a href="https://github.com/vaibhav174" class="icon brands fa-github" target="_blank"><span class="label">GitHub</span></a></li>
								</ul>
							</section>
						</section>
					</footer>


			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>